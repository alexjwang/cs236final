{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install aitextgen\n",
    "#!pip install sentence_transformers\n",
    "#!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive, copy_file_to_gdrive\n",
    "from aitextgen import aitextgen\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Clusters():\n",
    "    def __init__(self, n=10):\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.num_clusters = n\n",
    "\n",
    "    def train(self, txt):\n",
    "        corpus_embeddings = self.embedder.encode(txt.values)\n",
    "        self.kmeans = KMeans(n_clusters=self.num_clusters,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "        self.kmeans.fit(corpus_embeddings)\n",
    "        return self.kmeans.predict(corpus_embeddings)\n",
    "\n",
    "\n",
    "    def test(self, txt):\n",
    "        corpus_embeddings = self.embedder.encode(txt.values)\n",
    "        return self.kmeans.predict(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "train['text'] = \"Category: \" +  train.category + \" # \" + train.Rationale.str.replace('\"', '') + ' # ' + train.Problem\n",
    "train['prompt'] = \"Category: \" +  train.category + \" # \" + train.Rationale.str.replace('\"', '') + ' # '\n",
    "train['clean'] = train[['text']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "\n",
    "train.text.to_csv('rationale_train.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ncluster = 6\n",
    "physics = Clusters(ncluster)\n",
    "physics_train = physics.train(train[train.category == 'physics']['clean'])\n",
    "physics_test = physics.test(train[train.category == 'physics']['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_expl = train[train.category == 'physics']\n",
    "phys_expl['cluster'] = physics_test\n",
    "phys_expl['prompt'] = \"Category: \" + phys_expl.category + \" # \" + phys_expl.Rationale.str.replace('\"', '') + \" # \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_expl['generated'] = phys_expl.apply(lambda row: ai_0.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_expl['generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(ncluster):\n",
    "  texts.append(' '.join(phys_expl[phys_expl.cluster == i].clean.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ai = aitextgen()\n",
    "ai.train('rationale_train_0.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=3000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1,\n",
    "         output_dir='trained_model_0'\n",
    "         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ai = aitextgen()\n",
    "ai.train('rationale_train.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=3000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1, \n",
    "         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ai_5 = aitextgen()\n",
    "ai_5.train('rationale_train_5.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=3000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1,\n",
    "         output_dir='trained_model_5'\n",
    "         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ai_3 = aitextgen()\n",
    "ai_3.train('rationale_train_3.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=3000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1,\n",
    "         output_dir='trained_model_3'\n",
    "         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ai_4 = aitextgen()\n",
    "ai_4.train('rationale_train_4.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=3000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1,\n",
    "         output_dir='trained_model_4'\n",
    "         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_json('test.json')\n",
    "test['prompt'] = \"Category: \" + test.category + \" # \" + test.Rationale.str.replace('\"', '') + \" # \"\n",
    "test['text'] = \"Category: \" + test.category + \" # \" + test.Rationale.str.replace('\"', '') + \" # \" + test.Problem\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test = test[test.category == 'physics']\n",
    "phys_test['clean'] = phys_test[['text']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "phys_test['cluster'] = physics.test(phys_test)\n",
    "phys_test_0 = phys_test[phys_test.cluster == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ai = aitextgen(model_folder='trained_model', to_gpu=True)\n",
    "ai_0 = aitextgen(model_folder='trained_model_0', to_gpu=True)\n",
    "#ai_1 = aitextgen(model_folder='trained_model_1', to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_0['generated'] = phys_test_0.apply(lambda row: ai_0.generate(n=5, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_0['generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "class Comparer():\n",
    "    def __init__(self, l=1.0):\n",
    "        self.l = l\n",
    "        self.pattern = r'[0-9][0-9]*'\n",
    "\n",
    "    def convert(self, s):\n",
    "        nums = list(filter(lambda elem: elem.isnumeric(), s.split(' ')))\n",
    "        new_string = re.sub(self.pattern, 'Var', s)\n",
    "        return new_string, nums\n",
    "\n",
    "    def lossy_numbers(self, l1, l2):\n",
    "        loss = 0\n",
    "        for i in range(min(len(l1), len(l2))):\n",
    "            if l1[i] != l2[i]:\n",
    "                loss += 1\n",
    "        loss += max(len(l1), len(l2)) - min(len(l1), len(l2))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def loss_sentences(self, s1, s2):\n",
    "        m1, l1 = self.convert(s1)\n",
    "        m2, l2 = self.convert(s2)\n",
    "        return fuzz.partial_ratio(m1,m2)/100.0, self.l*(self.lossy_numbers(l1, l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = Comparer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_0['score'] = phys_test_0.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)\n",
    "phys_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_0['gpt_generated'] = phys_test_0.apply(lambda row: ai.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_0['gpt_generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_0['gpt_score'] = phys_test_0.apply(lambda row: c.loss_sentences(row['text'], row['gpt_generated'][0]), axis=1)\n",
    "phys_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_0.to_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_0 = pd.read_csv('output_0.csv').iloc[:, 1:]\n",
    "phys_test_0['score'] = phys_test_0.apply(lambda row: eval(row['score']), axis=1)\n",
    "phys_test_0['gpt_score'] = phys_test_0.apply(lambda row: eval(row['gpt_score']), axis=1)\n",
    "phys_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_0[['s_score', 'n_score']] = phys_test_0.score.tolist()\n",
    "phys_test_0[['s_gptscore', 'n_gptscore']] = phys_test_0.gpt_score.tolist()\n",
    "print(phys_test_0.s_score.mean())\n",
    "print(phys_test_0.s_gptscore.mean())\n",
    "print(phys_test_0.n_score.mean())\n",
    "print(phys_test_0.n_gptscore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_0.to_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test['gpt_generated'] = phys_test.apply(lambda row: ai.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test['gpt_score'] = phys_test.apply(lambda row: c.loss_sentences(row['text'], row['gpt_generated'][0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test.to_csv('physicsgenerated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test = pd.read_csv('physicsgenerated.csv')\n",
    "phys_test['gpt_score'] = phys_test.apply(lambda row: eval(row['gpt_score']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test[['s_gptscore', 'n_gptscore']] = phys_test['gpt_score'].tolist()\n",
    "print(phys_test.s_gptscore.mean())\n",
    "print(phys_test.n_gptscore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_1 = phys_test[phys_test.cluster == 1]\n",
    "phys_test_1['gpt_score'] = phys_test_1.apply(lambda row: c.loss_sentences(row['text'], row['gpt_generated'][0]), axis=1)\n",
    "phys_test[['s_gptscore', 'n_gptscore']] = phys_test['gpt_score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_1['generated'] = phys_test_1.apply(lambda row: ai_1.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_1['score'] = phys_test_1.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_1[['s_score', 'n_score']] = phys_test_1.score.tolist()\n",
    "phys_test_1.to_csv('output_1.csv')\n",
    "print(phys_test_1.s_score.mean())\n",
    "print(phys_test_1.n_score.mean())\n",
    "print(phys_test[phys_test.cluster == 1].s_gptscore.mean())\n",
    "print(phys_test[phys_test.cluster == 1].n_gptscore.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ai_2 = aitextgen(model_folder='trained_model_2', to_gpu=True)\n",
    "ai_3 = aitextgen(model_folder='trained_model_3', to_gpu=True)\n",
    "ai_4 = aitextgen(model_folder='trained_model_4', to_gpu=True)\n",
    "ai_5 = aitextgen(model_folder='trained_model_5', to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "phys_test_2 = phys_test[phys_test.cluster == 2]\n",
    "phys_test_2['generated'] = phys_test_2.apply(lambda row: ai_2.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_2['score'] = phys_test_2.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)\n",
    "phys_test_2[['s_score', 'n_score']] = phys_test_2.score.tolist()\n",
    "phys_test_2.to_csv('output_2.csv')\n",
    "print(phys_test_2.s_score.mean())\n",
    "print(phys_test_2.n_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_3 = phys_test[phys_test.cluster == 3]\n",
    "phys_test_3['generated'] = phys_test_3.apply(lambda row: ai_3.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_3['score'] = phys_test_3.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)\n",
    "phys_test_3[['s_score', 'n_score']] = phys_test_3.score.tolist()\n",
    "phys_test_3.to_csv('output_3.csv')\n",
    "print(phys_test_3.s_score.mean())\n",
    "print(phys_test_3.n_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_4 = phys_test[phys_test.cluster == 4]\n",
    "phys_test_4['generated'] = phys_test_4.apply(lambda row: ai_4.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_4['score'] = phys_test_4.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)\n",
    "phys_test_4[['s_score', 'n_score']] = phys_test_4.score.tolist()\n",
    "phys_test_4.to_csv('output_4.csv')\n",
    "print(phys_test_4.s_score.mean())\n",
    "print(phys_test_4.n_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "phys_test_5 = phys_test[phys_test.cluster == 5]\n",
    "phys_test_5['generated'] = phys_test_5.apply(lambda row: ai_5.generate(n=1, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "phys_test_5['score'] = phys_test_5.apply(lambda row: c.loss_sentences(row['text'], row['generated'][0]), axis=1)\n",
    "phys_test_5[['s_score', 'n_score']] = phys_test_5.score.tolist()\n",
    "phys_test_5.to_csv('output_5.csv')\n",
    "print(phys_test_5.s_score.mean())\n",
    "print(phys_test_5.n_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(phys_test_0.s_score.mean() + phys_test_1.s_score.mean() + phys_test_2.s_score.mean() + phys_test_3.s_score.mean() + \\\n",
    " phys_test_4.s_score.mean() + phys_test_5.s_score.mean()) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(phys_test_0.n_score.mean() + phys_test_1.n_score.mean() + phys_test_2.n_score.mean() + phys_test_3.n_score.mean() + \\\n",
    " phys_test_4.n_score.mean() + phys_test_5.n_score.mean()) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "class Comparer():\n",
    "    def __init__(self, l=1.0):\n",
    "        self.l = l\n",
    "        self.pattern = r'[0-9][0-9]*'\n",
    "\n",
    "    def convert(self, s):\n",
    "        nums = list(filter(lambda elem: elem.isnumeric(), s.split(' ')))\n",
    "        new_string = re.sub(self.pattern, 'Var', s)\n",
    "        return new_string, nums\n",
    "\n",
    "    def lossy_numbers(self, l1, l2):\n",
    "        loss = 0\n",
    "        for i in range(min(len(l1), len(l2))):\n",
    "            if l1[i] != l2[i]:\n",
    "                loss += 1\n",
    "        loss += max(len(l1), len(l2)) - min(len(l1), len(l2))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def loss_sentences(self, s1, s2):\n",
    "        m1, l1 = self.convert(s1)\n",
    "        m2, l2 = self.convert(s2)\n",
    "        #print(fuzz.ratio(m1,m2))\n",
    "        return -fuzz.partial_ratio(m1,m2)/100.0 + self.l*(self.lossy_numbers(l1, l2))\n",
    "    \n",
    "    def loss_sentences_output(self, s1, s2):\n",
    "        m1, l1 = self.convert(s1)\n",
    "        m2, l2 = self.convert(s2)\n",
    "        return -fuzz.partial_ratio(m1,m2)/100.0, self.l*(self.lossy_numbers(l1, l2))\n",
    "\n",
    "def get_val(s):\n",
    "    return list(filter(lambda elem: elem.isnumeric(), s.split(' ')))\n",
    "\n",
    "def get_label(s, l):\n",
    "    return list(map(lambda elem: 2*int(elem in l)-1, s.split(' ')))\n",
    "\n",
    "def generate_numbers(data, c):\n",
    "    data['labels'] = data.apply(lambda r: c.loss_sentences(r['clean'], r['generated_clean']) , axis=1)\n",
    "    #data['labels'] = np.log(data['labels'])\n",
    "    data['labels'] = data['labels']/data['labels'].max()\n",
    "    return data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv('physicsgenerated.csv')\n",
    "output_0 = pd.read_csv('output_0.csv')\n",
    "output_1 = pd.read_csv('output_1.csv')\n",
    "output_2 = pd.read_csv('output_2.csv')\n",
    "output_3 = pd.read_csv('output_3.csv')\n",
    "output_4 = pd.read_csv('output_4.csv')\n",
    "output_5 = pd.read_csv('output_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build a model\n",
    "inputs = layers.Input(shape=(384,))\n",
    "layer1 = layers.Dense(128, activation='relu')(inputs)\n",
    "layer2 = layers.Dense(128)(layer1)\n",
    "layer3 = keras.layers.LeakyReLU(alpha=0.3)(layer2)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(layer3)\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)\n",
    "print('Model Built')\n",
    "# Define custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated_clean'] = output_0[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "output_0['generated_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated_clean'] = output_0[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_0, Comparer())\n",
    "output_0.to_csv('output_0.csv')\n",
    "\n",
    "output_1['generated_clean'] = output_1[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_1, Comparer())\n",
    "output_1.to_csv('output_1.csv')\n",
    "\n",
    "output_2['generated_clean'] = output_2[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_2, Comparer())\n",
    "output_2.to_csv('output_2.csv')\n",
    "\n",
    "output_3['generated_clean'] = output_3[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_3, Comparer())\n",
    "output_3.to_csv('output_3.csv')\n",
    "\n",
    "output_4['generated_clean'] = output_4[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_4, Comparer())\n",
    "output_4['loss'] = labels\n",
    "output_4.to_csv('output_4.csv')\n",
    "\n",
    "output_5['generated_clean'] = output_5[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_5, Comparer())\n",
    "output_5.to_csv('output_5.csv')\n",
    "\n",
    "output['generated_clean'] = output[['gpt_generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output, Comparer())\n",
    "output.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated_clean'] = output_0[['generated']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "labels = generate_numbers(output_0, Comparer())\n",
    "output_0.to_csv('output_0.csv')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse', # Call the loss function with the selected layer\n",
    "              metrics=['accuracy'])\n",
    "print('Model  compiled')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.fit(embedder.encode(output_0['generated_clean'].values), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0 = pd.read_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated'] = output_0.apply(lambda row: ai_0.generate(n=5, prompt=row['prompt'], repetition_penalty=1.1, return_as_list=True, max_length=500), axis=1)\n",
    "output_0[['generated_0', 'generated_1', 'generated_2', 'generated_3', 'generated_4']] = \\\n",
    "    output_0['generated'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated_clean_0'] = output_0[['generated_0']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "output_0['generated_clean_1'] = output_0[['generated_1']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "output_0['generated_clean_2'] = output_0[['generated_2']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "output_0['generated_clean_3'] = output_0[['generated_3']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))\n",
    "output_0['generated_clean_4'] = output_0[['generated_4']].applymap(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords=nltk.corpus.stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0.to_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0['generated_clean_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('.')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "min_score = 10000\n",
    "min_col = ''\n",
    "for col in ['generated_clean_0', 'generated_clean_1', 'generated_clean_2', 'generated_clean_3', 'generated_clean_4']:\n",
    "    col_name = col + '_score'\n",
    "    output_0[col_name] = model.predict(embedder.encode(output_0[col].values))\n",
    "cols = ['generated_clean_0_score', 'generated_clean_1_score', 'generated_clean_2_score', 'generated_clean_3_score', 'generated_clean_4_score']\n",
    "output_0['min_col'] = output_0[cols].idxmin(axis=1)\n",
    "output_0['min_col'] = output_0.apply(lambda x: x['min_col'].strip('_score'), axis=1)\n",
    "output_0['output'] = output_0.apply(lambda x: x[x['min_col']], axis=1)\n",
    "output_0['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_0.to_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c = Comparer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_0['score'] = output_0.apply(lambda row: c.loss_sentences_output(row['clean'], row['output']), axis=1)\n",
    "output_0[['s_score', 'n_score']] = output_0.score.tolist()\n",
    "output_0.to_csv('output_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1 = \"a rectangular floor that measures 15 meters by 18 meters is to be covered with carpet squares that each measure 10 meters by 3 meters. if the carpet squares cost $ 12 apiece, how much will the carpet squares cost?\"\n",
    "s2 = \"a rectangular floor that measures 15 meters by 18 meters is to be covered with carpet squares that each measure 3 meters by 3 meters . if the carpet squares cost $12 apiece , what is the total cost for the number of carpet squares needed to cover the floor ?\"\n",
    "c.loss_sentences(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
